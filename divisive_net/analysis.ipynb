{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "import divisivenormalization.analysis as analysis\n",
    "import divisivenormalization.utils as helpers\n",
    "from divisivenormalization.data import Dataset, MonkeySubDataset\n",
    "\n",
    "helpers.config_ipython()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "# adjust sns paper context rc parameters\n",
    "font_size = 8\n",
    "rc_dict = {\n",
    "    \"font.size\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"figure.figsize\": (helpers.cm2inch(8), helpers.cm2inch(8)),\n",
    "    \"figure.dpi\": 300,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"savefig.transparent\": True,\n",
    "    \"savefig.bbox_inches\": \"tight\",\n",
    "}\n",
    "sns.set_context(\"paper\", rc=rc_dict)\n",
    "\n",
    "\n",
    "class args:\n",
    "    num_best = 10\n",
    "    num_val = 10\n",
    "    fname_best_csv = \"df_best.csv\"\n",
    "    fname_val_csv = \"df_val.csv\"\n",
    "    weights_path = \"weights\"\n",
    "    train_logs_path = \"train_logs\"\n",
    "    orientation_binsize = np.deg2rad(10)\n",
    "    stim_full_size = 140  # full size of stimulus w/o subsampling and cropping\n",
    "    stim_subsample = 2\n",
    "    nonspecific_path = (\n",
    "        \"/projects/burg2021_learning-divisive-normalization/nonspecific_divisive_net\"\n",
    "    )\n",
    "    subunit_path = \"/projects/burg2021_learning-divisive-normalization/subunit_net\"\n",
    "    cnn3_path = \"/projects/burg2021_learning-divisive-normalization/cnn3\"\n",
    "    surround_path_dict = {\n",
    "        3: \"/projects/burg2021_learning-divisive-normalization/divisive_3x3_surround_net\",\n",
    "        5: \"/projects/burg2021_learning-divisive-normalization/divisive_5x5_surround_net\",\n",
    "        7: \"/projects/burg2021_learning-divisive-normalization/divisive_7x7_surround_net\",\n",
    "    }\n",
    "    oriented_threshold = 0.125\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"results.csv\")\n",
    "# Save a simplified version of the csv file, sorted by validation set performance\n",
    "df_plain = helpers.simplify_df(results_df)\n",
    "df_plain.to_csv(\"results_plain.csv\")\n",
    "\n",
    "data_dict = Dataset.get_clean_data()\n",
    "data = MonkeySubDataset(data_dict, seed=1000, train_frac=0.8, subsample=2, crop=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split into set of best models and validation models\n",
    " Use the 10 best models for analysis. Use the best 11-20 models to tune analysis algorithms.\n",
    " Split the csv files accordingly. Also, extract some weights to be used for later analysis and save\n",
    " them as pickle. As this operation requires model loading, we do it only if it was not done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_best = pd.read_csv(args.fname_best_csv)\n",
    "    logging.info(\"loaded data from \" + args.fname_best_csv)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    df_best = df_plain[0 : args.num_best].copy()\n",
    "\n",
    "    fev_lst = []\n",
    "    for i in range(args.num_best):\n",
    "        run_no = df_best.iloc[i][\"run_no\"]\n",
    "        logging.info(\"load run no \" + str(run_no))\n",
    "        model = helpers.load_dn_model(run_no, results_df, data, args.train_logs_path)\n",
    "\n",
    "        fev = model.evaluate_fev_testset()\n",
    "        fev_lst.append(fev)\n",
    "\n",
    "        feve = model.evaluate_fev_testset_per_neuron()\n",
    "        var_explained, explainable_var = model.evaluate_ve_testset_per_neuron()\n",
    "        helpers.pkl_dump(feve, run_no, \"feve.pkl\", args.weights_path)\n",
    "\n",
    "        # get weights and normalization input\n",
    "        (\n",
    "            features_chanfirst,\n",
    "            p,\n",
    "            pooled,\n",
    "            readout_feat,\n",
    "            u,\n",
    "            v,\n",
    "            dn_exponent,\n",
    "        ) = helpers.get_weights(model)\n",
    "\n",
    "        norm_input = analysis.norm_input(pooled, p)\n",
    "\n",
    "        helpers.pkl_dump(\n",
    "            features_chanfirst, run_no, \"features_chanfirst.pkl\", args.weights_path\n",
    "        )\n",
    "        helpers.pkl_dump(p, run_no, \"p.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(pooled, run_no, \"pooled.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(norm_input, run_no, \"norm_input.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(readout_feat, run_no, \"readout_feat_w.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(u, run_no, \"u.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(v, run_no, \"v.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(dn_exponent, run_no, \"dn_exponent.pkl\", args.weights_path)\n",
    "\n",
    "    df_best[\"fev\"] = fev_lst\n",
    "    df_best.to_csv(args.fname_best_csv)\n",
    "\n",
    "\n",
    "try:\n",
    "    df_val = pd.read_csv(\"df_val.csv\")\n",
    "    logging.info(\"loaded data from \" + args.fname_val_csv)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    df_val = df_plain[args.num_best : args.num_best + args.num_val].copy()\n",
    "\n",
    "    fev_lst = []\n",
    "    for i in range(args.num_val):\n",
    "        run_no = df_val.iloc[i][\"run_no\"]\n",
    "        logging.info(\"load run no \" + str(run_no))\n",
    "        model = helpers.load_dn_model(run_no, results_df, data, args.train_logs_path)\n",
    "\n",
    "        fev = model.evaluate_fev_testset()\n",
    "        fev_lst.append(fev)\n",
    "\n",
    "        features_chanfirst = helpers.get_weights(model)[0]\n",
    "        helpers.pkl_dump(\n",
    "            features_chanfirst, run_no, \"features_chanfirst.pkl\", args.weights_path\n",
    "        )\n",
    "\n",
    "    df_val[\"fev\"] = fev_lst\n",
    "    df_val.to_csv(args.fname_val_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Compare model performance\n",
    " *For this to work, you first have to run the cell \"Get and save FEV performance on test set\"\n",
    " in the cnn3, nonspecific_divisive_net, and subunit_net analysis jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "fev_dn = df_best.fev.values\n",
    "fev_nonspecific = pd.read_csv(\n",
    "    os.path.join(args.nonspecific_path, \"df_best.csv\")\n",
    ").fev.values\n",
    "fev_subunit = pd.read_csv(os.path.join(args.subunit_path, \"df_best.csv\")).fev.values\n",
    "fev_cnn3 = pd.read_csv(os.path.join(args.cnn3_path, \"df_best.csv\")).fev.values\n",
    "\n",
    "fev_lst = 100 * np.array([fev_subunit, fev_nonspecific, fev_dn, fev_cnn3])\n",
    "fev_stats = analysis.compute_fev_summary_stats(fev_lst)\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(6), helpers.cm2inch(8)))\n",
    "x = np.arange(len(fev_stats[\"mean\"]))\n",
    "plt.scatter(\n",
    "    x,\n",
    "    fev_stats[\"mean\"],\n",
    "    color=[\"grey\", \"xkcd:Bluegreen\", \"xkcd:Blue\", \"grey\"],\n",
    "    marker=\"_\",\n",
    "    linewidths=[0.01] * 4,\n",
    ")\n",
    "plt.errorbar(\n",
    "    x, fev_stats[\"mean\"], yerr=fev_stats[\"sem\"], fmt=\"none\", color=\"xkcd:black\"\n",
    ")\n",
    "plt.xticks(\n",
    "    x,\n",
    "    [\"Subunit\", \"Nonspecific DN\", \"Specific DN\", \"Black-box CNN\"],\n",
    "    rotation=45,\n",
    "    horizontalalignment=\"right\",\n",
    ")\n",
    "plt.yticks(ticks=fev_stats[\"mean\"], labels=np.round(fev_stats[\"mean\"], 1))\n",
    "plt.ylabel(\"Absolute accuracy (% FEV)\")\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Mean FEV\", np.round(fev_stats[\"mean\"], 1))\n",
    "print(\"SEM\", np.round(fev_stats[\"sem\"], 1))\n",
    "\n",
    "fev = fev_stats[\"mean\"]\n",
    "percent = (fev - fev[0]) / (fev[-1] - fev[0]) * 100\n",
    "print(\"Percentage scale\", np.round(percent, 0))\n",
    "\n",
    "sem = fev_stats[\"sem\"]\n",
    "percent_sem = sem / (fev[-1] - fev[0]) * 100\n",
    "print(\"Percentage scale SEM\", np.round(percent_sem, 1))\n",
    "print()\n",
    "\n",
    "ci = fev_stats[\"conf_int\"]\n",
    "print(\"Confidence intervals:\")\n",
    "for c, shapiro_reject, name in zip(\n",
    "    ci,\n",
    "    fev_stats[\"shapiro_reject\"],\n",
    "    [\"fev_subunit\", \"fev_nonspecific\", \"fev_dn\", \"fev_cnn3\"],\n",
    "):\n",
    "    percent = (c - fev[0]) / (fev[-1] - fev[0]) * 100\n",
    "    percent = percent.squeeze()\n",
    "    p_pm = (percent[1] - percent[0]) / 2\n",
    "    c = np.array(c).squeeze()\n",
    "    c_pm = (c[1] - c[0]) / 2\n",
    "    print()\n",
    "    print(name)\n",
    "    print(\n",
    "        \"Confidence interval:\",\n",
    "        np.round(c, 1),\n",
    "        \"Plus/minus:\",\n",
    "        np.round(c_pm, 1),\n",
    "        \"Percentage scale:\",\n",
    "        np.round(percent, 1),\n",
    "        \"Plus/minus (percentage):\",\n",
    "        np.round(p_pm, 0),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests\n",
    "\n",
    "top_idx = 0\n",
    "feve = {}\n",
    "feve_surr = {}\n",
    "var_explained = {}\n",
    "explainable_var = {}\n",
    "\n",
    "run_no = df_best.iloc[top_idx].run_no\n",
    "feve[\"dn\"] = helpers.pkl_load(run_no, \"feve.pkl\", args.weights_path)\n",
    "\n",
    "run_no = (\n",
    "    pd.read_csv(os.path.join(args.nonspecific_path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    ")\n",
    "feve[\"dn-non-specific\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(args.nonspecific_path, args.weights_path)\n",
    ")\n",
    "\n",
    "run_no = (\n",
    "    pd.read_csv(os.path.join(args.subunit_path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    ")\n",
    "feve[\"subunit\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(args.subunit_path, args.weights_path)\n",
    ")\n",
    "\n",
    "run_no = pd.read_csv(os.path.join(args.cnn3_path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    "feve[\"cnn3\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(args.cnn3_path, args.weights_path)\n",
    ")\n",
    "\n",
    "\n",
    "feve_surr[\"dn\"] = feve[\"dn\"]\n",
    "\n",
    "path = args.surround_path_dict[3]\n",
    "run_no = pd.read_csv(os.path.join(path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    "feve_surr[\"dn3\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(path, args.weights_path)\n",
    ")\n",
    "\n",
    "path = args.surround_path_dict[5]\n",
    "run_no = pd.read_csv(os.path.join(path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    "feve_surr[\"dn5\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(path, args.weights_path)\n",
    ")\n",
    "\n",
    "path = args.surround_path_dict[7]\n",
    "run_no = pd.read_csv(os.path.join(path, \"df_best.csv\")).iloc[top_idx].run_no\n",
    "feve_surr[\"dn7\"] = helpers.pkl_load(\n",
    "    run_no, \"feve.pkl\", os.path.join(path, args.weights_path)\n",
    ")\n",
    "\n",
    "\n",
    "# Compare center models\n",
    "\n",
    "num_models = len(feve.keys())\n",
    "num_neurons = len(feve[\"dn\"])\n",
    "neuron_id_lst = [i for i in range(num_neurons)] * num_models\n",
    "\n",
    "feve_lst = []\n",
    "model_lst = []\n",
    "for k, v in feve.items():\n",
    "    feve_lst.extend(v)\n",
    "    model_lst.extend(k for i in range(num_neurons))\n",
    "feve_df = pd.DataFrame(dict(neuron_id=neuron_id_lst, feve=feve_lst, model=model_lst))\n",
    "\n",
    "mod = MultiComparison(feve_df.feve, feve_df.model)\n",
    "res = mod.allpairtest(stats.wilcoxon, method=\"holm\")\n",
    "print('Center models')\n",
    "print(res[0])\n",
    "print(\"Corrected p values\")\n",
    "print(res[1][2])\n",
    "\n",
    "\n",
    "# Compare surround models\n",
    "\n",
    "num_neurons = len(feve_surr[\"dn3\"])\n",
    "num_models = len(feve_surr.keys())\n",
    "neuron_id_lst = [i for i in range(num_neurons)] * num_models\n",
    "\n",
    "feve_lst = []\n",
    "model_lst = []\n",
    "for k, v in feve_surr.items():\n",
    "    feve_lst.extend(v)\n",
    "    model_lst.extend(k for i in range(num_neurons))\n",
    "feve_surr_df = pd.DataFrame(\n",
    "    dict(neuron_id=neuron_id_lst, feve=feve_lst, model=model_lst)\n",
    ")\n",
    "\n",
    "mod = MultiComparison(feve_surr_df.feve, feve_surr_df.model)\n",
    "res = mod.allpairtest(stats.wilcoxon, method=\"holm\")\n",
    "print('\\n\\nSurround models')\n",
    "print(res[0])\n",
    "print(\"Corrected p values\")\n",
    "print(res[1][2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot distribution of divisive normalization exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = []\n",
    "unori = []\n",
    "for i in range(args.num_best):\n",
    "    run_no = df_best.iloc[i].run_no\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    angles = analysis.angles_circ_var(features, threshold=args.oriented_threshold)\n",
    "    n = helpers.pkl_load(run_no, \"dn_exponent.pkl\", args.weights_path)\n",
    "    n = n.squeeze()\n",
    "    unori_mask = np.isnan(angles)\n",
    "    ori.extend(list(n[~unori_mask].flatten()))\n",
    "    unori.extend(list(n[unori_mask].flatten()))\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(8), helpers.cm2inch(8 / 8 * 6)))\n",
    "plt.hist([ori, unori], bins=15, lw=0, color=[\"xkcd:blue\", \"xkcd:lightblue\"])\n",
    "plt.legend([\"Ori.\", \"Unori.\"], loc=\"upper left\")\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel(\"Values of exponents $n_l$\")\n",
    "plt.ylabel(\"No. of exponents $n_l$\")\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n = []\n",
    "n.extend(ori)\n",
    "n.extend(unori)\n",
    "print(\"mean\", np.round(np.mean(n), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Validate feature orientation sorting\n",
    " Use the 11-20 best models for this (in terms of validation set performance). Then perform the\n",
    " actual analysis on the best 10 models. Filters marked by red axis are considered unoriented by our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(args.num_val, 32, figsize=(32, args.num_val))\n",
    "for i, axrow in zip(range(args.num_val), axes):\n",
    "    run_no = df_val.iloc[i][\"run_no\"]\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    angles = analysis.angles_circ_var(features, threshold=args.oriented_threshold)\n",
    "    idx_pref = np.argsort(angles)\n",
    "    features = features[idx_pref]\n",
    "    angles = angles[idx_pref]\n",
    "\n",
    "    for ax, feat, angle in zip(axrow, features, angles):\n",
    "        vmax = np.max(np.abs(feat))\n",
    "        vmin = -vmax\n",
    "        ax.imshow(feat, vmax=vmax, vmin=vmin, cmap=\"gray\")\n",
    "        ax.tick_params(\n",
    "            which=\"both\", bottom=False, labelbottom=False, left=False, labelleft=False\n",
    "        )\n",
    "\n",
    "        if np.isnan(angle):\n",
    "            color = \"red\"\n",
    "            ax.spines[\"bottom\"].set_color(color)\n",
    "            ax.spines[\"top\"].set_color(color)\n",
    "            ax.spines[\"right\"].set_color(color)\n",
    "            ax.spines[\"left\"].set_color(color)\n",
    "\n",
    "plt.suptitle(\"treshold \" + str(args.oriented_threshold))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Matrix plot showing the structure of DN for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "top_idx = 0\n",
    "run_no = df_best.iloc[top_idx].run_no\n",
    "norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "\n",
    "angles = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "idx_pref = np.argsort(angles)\n",
    "# Put 0st filter to the end of the oriented ones\n",
    "idx_pref = np.concatenate((idx_pref[1:19], idx_pref[0:1], idx_pref[19:]))\n",
    "\n",
    "# sort\n",
    "features = features[idx_pref]\n",
    "angles = angles[idx_pref]\n",
    "norm_input = norm_input[idx_pref][:, idx_pref]\n",
    "\n",
    "# sort unoriented according to norm input (dark color to the right)\n",
    "unor_mask = np.isnan(angles)\n",
    "norm_input_unor = norm_input[unor_mask][:, unor_mask]\n",
    "norm_input_unor = np.sum(norm_input_unor, axis=1)\n",
    "idx_unor = np.argsort(norm_input_unor)\n",
    "features[unor_mask] = features[unor_mask][idx_unor]\n",
    "angles[unor_mask] = angles[unor_mask][idx_unor]\n",
    "norm_input[unor_mask] = norm_input[unor_mask][idx_unor]\n",
    "norm_input[:, unor_mask] = norm_input[:, unor_mask][:, idx_unor]\n",
    "\n",
    "oriented_bools = np.logical_not(unor_mask)\n",
    "angle_diff = analysis.angle_diff(angles)\n",
    "\n",
    "# matrix plot\n",
    "figsize = (helpers.cm2inch(5), helpers.cm2inch(5))\n",
    "fig = analysis.plot_contribution_matrix_chan_first(\n",
    "    norm_input,\n",
    "    features,\n",
    "    index_permutation_lst=np.arange(32),\n",
    "    angle_difference=angle_diff,\n",
    "    oriented_bools=oriented_bools,\n",
    "    figsize=figsize,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# color bar\n",
    "vmax = np.max(norm_input[oriented_bools][:, oriented_bools])\n",
    "vmin = 0\n",
    "fig, ax = plt.subplots(figsize=(6, 1))\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "cmap = matplotlib.cm.Blues\n",
    "norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "ticks = np.round(np.linspace(vmin, vmax - 0.01, 4), 2)\n",
    "cb1 = matplotlib.colorbar.ColorbarBase(\n",
    "    ax, cmap=cmap, norm=norm, orientation=\"horizontal\", ticks=ticks\n",
    ")\n",
    "cb1.set_label(\"Normalization input\")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Similarly oriented features contribute stronger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_input_lst, dissim_input_lst = [], []\n",
    "for i in range(args.num_best):\n",
    "    run_no = df_best.iloc[i].run_no\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "\n",
    "    angles = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "    angles_diff = analysis.angle_diff(angles)\n",
    "    unor_mask, sim_mask, dissim_mask = analysis.orientation_masks(angles_diff)\n",
    "    sim_input = np.sum(norm_input[sim_mask])\n",
    "    dissim_input = np.sum(norm_input[dissim_mask])\n",
    "\n",
    "    sim_input_lst.append(sim_input)\n",
    "    dissim_input_lst.append(dissim_input)\n",
    "\n",
    "sim_err = stats.sem(sim_input_lst, ddof=0)\n",
    "dissim_err = stats.sem(dissim_input_lst, ddof=0)\n",
    "\n",
    "fractions = [s / d for s, d in zip(sim_input_lst, dissim_input_lst)]\n",
    "fraction_err = stats.sem(fractions, ddof=0)\n",
    "mean = np.average(fractions)\n",
    "conf_int = analysis.compute_confidence_interval(fractions)\n",
    "\n",
    "print(\"Similar normalization input divided by dissimilar input\", np.round(mean, 2))\n",
    "print(\"Confidence interval\", np.round(conf_int, 2))\n",
    "print(\"Plus/minus\", np.round(mean - conf_int[0], 2))\n",
    "print(stats.wilcoxon(sim_input_lst, dissim_input_lst))\n",
    "print(\"Cohen's d\", np.round(analysis.cohens_d(sim_input_lst, dissim_input_lst), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_input_lst, dissim_input_lst = [], []\n",
    "for i in range(args.num_best):\n",
    "    run_no = df_best.iloc[i].run_no\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "\n",
    "    cos_sim = np.zeros((features.shape[0], features.shape[0]))\n",
    "    for i, a in enumerate(features):\n",
    "        for j, b in enumerate(features):\n",
    "            cos_sim[i, j] = analysis.cosine_similarity(a, b)\n",
    "    \n",
    "    crit = 0\n",
    "    sim_mask = cos_sim > crit\n",
    "    dissim_mask = cos_sim <= crit\n",
    "    sim_input = np.sum(norm_input[sim_mask])\n",
    "    dissim_input = np.sum(norm_input[dissim_mask])\n",
    "    sim_input_lst.append(sim_input)\n",
    "    dissim_input_lst.append(dissim_input)\n",
    "\n",
    "sim_err = stats.sem(sim_input_lst, ddof=0)\n",
    "dissim_err = stats.sem(dissim_input_lst, ddof=0)\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(4), helpers.cm2inch(6)))\n",
    "ax = sns.barplot(\n",
    "    np.arange(2),\n",
    "    [np.average(sim_input_lst), np.average(dissim_input_lst)],\n",
    "    yerr=[sim_err, dissim_err],\n",
    "    palette=[\"xkcd:blue\", \"grey\"],\n",
    ")\n",
    "plt.xlabel(\"Cosine similarity\")\n",
    "plt.ylabel(\"Normalization input (a.u.)\")\n",
    "plt.xticks(plt.xticks()[0], [r\"$\\geq$ 0\", r\"< 0\"])\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fractions = [s / d for s, d in zip(sim_input_lst, dissim_input_lst)]\n",
    "mean = np.average(fractions)\n",
    "conf_int = analysis.compute_confidence_interval(fractions)\n",
    "print(\"Similar normalization input divided by dissimilar input\", np.round(mean, 2))\n",
    "print(\"Confidence interval\", np.round(conf_int, 2))\n",
    "print(\"Plus/minus\", np.round(mean - conf_int[0], 2))\n",
    "print(stats.wilcoxon(sim_input_lst, dissim_input_lst))\n",
    "print(\"Cohens'd\", np.round(analysis.cohens_d(sim_input_lst, dissim_input_lst), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot normalization input vs. orientation difference (binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over models, collect activations and according angle differences\n",
    "contrib_model_lst = []\n",
    "num_featpair_model_lst = []\n",
    "\n",
    "for i in range(args.num_best):\n",
    "    model_df = df_best.iloc[i]\n",
    "    run_no = model_df.run_no\n",
    "    norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    a = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "    a_diff = analysis.angle_diff(a)\n",
    "    unor_mask = np.isnan(a_diff)\n",
    "    unor_contrib = np.sum(norm_input[unor_mask])\n",
    "\n",
    "    mask_lst = []\n",
    "    contrib_lst = []\n",
    "    num_featpair_lst = []\n",
    "    nbins = int(np.ceil(np.deg2rad(90) / args.orientation_binsize))\n",
    "    crit_angles = args.orientation_binsize * np.arange(nbins)\n",
    "    for idx, angle_crit in enumerate(crit_angles):\n",
    "        # first bins (last bin follows as it is cornercase):\n",
    "        if idx != (crit_angles.shape[0] - 1):\n",
    "            mask = np.logical_and(\n",
    "                (a_diff < angle_crit + args.orientation_binsize),\n",
    "                np.logical_not(unor_mask),\n",
    "            )\n",
    "        else:\n",
    "            # last bin: include also the maximal difference\n",
    "            mask = np.logical_and(\n",
    "                (a_diff <= angle_crit + 2 * args.orientation_binsize),\n",
    "                np.logical_not(unor_mask),\n",
    "            )\n",
    "\n",
    "        mask = np.logical_and(mask, (a_diff >= angle_crit))\n",
    "        mask_lst.append(mask)\n",
    "        contrib = np.sum(norm_input[mask])\n",
    "        contrib_lst.append(contrib)\n",
    "        num_featurepairs = np.sum(mask)\n",
    "        num_featpair_lst.append(num_featurepairs)\n",
    "\n",
    "    contrib_model_lst.append(contrib_lst)\n",
    "    num_featpair_model_lst.append(num_featpair_lst)\n",
    "\n",
    "contrib_model_arr = np.array(contrib_model_lst)\n",
    "contrib_avg = np.average(contrib_model_arr, 0)\n",
    "contrib_std = np.std(contrib_model_arr, 0)\n",
    "num_feat_pair_avg = np.average(num_featpair_model_lst, 0)\n",
    "\n",
    "a_diff_bins = np.arange(contrib_avg.shape[0]) * args.orientation_binsize\n",
    "a_diff_bins = np.rad2deg(a_diff_bins)\n",
    "a_diff_bins = np.round(a_diff_bins, 0)\n",
    "xval = np.rad2deg(args.orientation_binsize) * np.arange(nbins) + (\n",
    "    np.rad2deg(args.orientation_binsize) / 2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(8), helpers.cm2inch(8)))\n",
    "plt.plot(xval, contrib_avg, \"-ok\")\n",
    "plt.fill_between(\n",
    "    xval, contrib_avg - contrib_std, contrib_avg + contrib_std, color=\"grey\", alpha=0.4\n",
    ")\n",
    "xticks = np.rad2deg(args.orientation_binsize) * np.arange(nbins + 1)\n",
    "plt.xticks(ticks=xticks)\n",
    "plt.xlabel(\"Orientation difference (deg)\")\n",
    "plt.ylabel(\"Normalization input (a.u.)\")\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fractions = contrib_model_arr[:, 0] / contrib_model_arr[:, -1]\n",
    "mean = np.average(fractions)\n",
    "conf_int = analysis.compute_confidence_interval(fractions)\n",
    "print(\"First bin divided by last bin\", np.round(mean, 2))\n",
    "print(\"Confidence interval\", np.round(conf_int, 2))\n",
    "print(\"Plus/minus\", np.round(mean - conf_int[0], 2))\n",
    "print(stats.wilcoxon(contrib_model_arr[:, 0], contrib_model_arr[:, -1]))\n",
    "print(\n",
    "    \"Cohens'd\",\n",
    "    np.round(analysis.cohens_d(contrib_model_arr[:, 0], contrib_model_arr[:, -1]), 1),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Linear regression analysis\n",
    " on all normalization input vs. orientation difference pairs of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df_best.iloc[0]\n",
    "run_no = model_df.run_no\n",
    "norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "a = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "a_diff = analysis.angle_diff(a)\n",
    "norm_input = norm_input.flatten()\n",
    "a_diff = a_diff.flatten()\n",
    "\n",
    "# remove nan's (unoriented filters)\n",
    "norm_input = norm_input[~np.isnan(a_diff)]\n",
    "a_diff = a_diff[~np.isnan(a_diff)]\n",
    "a_diff = np.rad2deg(a_diff)\n",
    "\n",
    "reg = stats.linregress(a_diff, norm_input)\n",
    "print(reg)\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(8), helpers.cm2inch(8)))\n",
    "plt.plot(a_diff, norm_input, \".\", color=\"xkcd:grey\", label=\"Data\")\n",
    "x = np.linspace(np.min(a_diff), np.max(a_diff))\n",
    "plt.plot(x, x * reg[0] + reg[1], color=\"xkcd:blue\", label=\"Linear fit\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Orientation difference (deg)\")\n",
    "plt.ylabel(\"Normalization input (a.u.)\")\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot normalization input for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "run_no = df_best.iloc[0][\"run_no\"]\n",
    "contrib = helpers.pkl_load(\n",
    "    run_no, \"norm_input.pkl\", args.weights_path\n",
    ")  # shape: out-chan, in-chan\n",
    "features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "angles = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "\n",
    "# use only oriented features\n",
    "mask = np.logical_not(np.isnan(angles))\n",
    "contrib = contrib[mask][:, mask]\n",
    "features = features[mask]\n",
    "angles = angles[mask]\n",
    "\n",
    "a_diff = analysis.angle_diff(angles)\n",
    "_, sim_mask, dissim_mask = analysis.orientation_masks(a_diff, angle_crit=45)\n",
    "sim_contrib = contrib * sim_mask\n",
    "dissim_contrib = contrib * dissim_mask\n",
    "\n",
    "# Sum contributions over in channel, for each out channel\n",
    "same_val = np.sum(sim_contrib, -1)\n",
    "diff_val = np.sum(dissim_contrib, -1)\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(8), helpers.cm2inch(8)))\n",
    "plt.scatter(diff_val, same_val, color=\"black\", s=5)  # dots for figure post-processing\n",
    "# Plot identity\n",
    "max_val = np.max([diff_val, same_val])\n",
    "plt.plot(np.linspace(0, max_val), np.linspace(0, max_val), \"Grey\")\n",
    "\n",
    "# Plot features\n",
    "X = [(diff_val[i], same_val[i]) for i in range(len(diff_val))]  # get coordinates\n",
    "for i in range(len(diff_val)):\n",
    "    image = features[i]\n",
    "    # normalize features to symmetric color scale\n",
    "    vmax = np.max(np.abs(image))\n",
    "    vmin = -vmax\n",
    "    norm = matplotlib.colors.Normalize(vmin, vmax)\n",
    "    imagebox = offsetbox.AnnotationBbox(\n",
    "        offsetbox.OffsetImage(image, cmap=plt.cm.gray_r, norm=norm, zoom=0.8),\n",
    "        X[i],\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.gca().add_artist(imagebox)\n",
    "\n",
    "plt.xlabel(\"Norm. input from dissimilarly oriented features\")\n",
    "plt.ylabel(\"Norm. input from similarly oriented features\")\n",
    "plt.xticks(np.arange(0, 1.21, 0.3))\n",
    "plt.yticks(np.arange(0, 1.21, 0.3))\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Histogram of feature readout weights\n",
    " For ten best performing models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout_feat_lst = []\n",
    "for i in range(args.num_best):\n",
    "    run_no = df_best.iloc[i].run_no\n",
    "    readout_feat_weight = helpers.pkl_load(\n",
    "        run_no, \"readout_feat_w.pkl\", args.weights_path\n",
    "    )\n",
    "    readout_feat_lst.append(readout_feat_weight)\n",
    "\n",
    "readout_features = np.array(readout_feat_lst)\n",
    "rfs_chan_norm = np.linalg.norm(\n",
    "    readout_features, axis=1, keepdims=True\n",
    ")  # normalize over channels\n",
    "readout_features = readout_features / rfs_chan_norm\n",
    "readout_features = np.average(readout_features, axis=-1)  # avg over neurons\n",
    "readout_features = readout_features.flatten()\n",
    "\n",
    "plt.figure(figsize=(helpers.cm2inch(8), helpers.cm2inch(8 / 8 * 6)))\n",
    "plt.hist(readout_features, bins=15, color=\"Grey\", edgecolor=\"Grey\", linewidth=1)\n",
    "plt.xlim(left=0)\n",
    "sns.despine(trim=True, offset=5)\n",
    "plt.xlabel(\"Avg. feature readout weight (a.u.)\")\n",
    "plt.ylabel(\"No. of features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "coeff_of_variation = np.std(readout_features) / np.mean(readout_features)\n",
    "print(\"Coefficient of variation\", np.round(coeff_of_variation, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model performance vs. size of normalization pool\n",
    " *For this to work, you first have to run the cell \"Get and save FEV performance on test set\"\n",
    " in the divisive_3x3_surround_net, divisive_5x5_surround_net, and divisive_7x7_surround_net analysis jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fev_dict = {}\n",
    "max_valset_fev = np.empty((len(args.surround_path_dict) + 1,))\n",
    "\n",
    "# models with surround\n",
    "for i, (surround_size, path) in enumerate(args.surround_path_dict.items()):\n",
    "    fev_vals = pd.read_csv(os.path.join(path, \"df_best.csv\")).fev.values\n",
    "    fev_dict[surround_size] = fev_vals\n",
    "    max_valset_fev[i + 1] = fev_vals[0]\n",
    "\n",
    "# model w/o surround\n",
    "fev_vals = pd.read_csv(os.path.join(\"df_best.csv\")).fev.values\n",
    "fev_dict[1] = fev_vals\n",
    "max_valset_fev[0] = fev_vals[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8/2.54, 8/2.54))\n",
    "y = np.array(fev_dict[1][1:])\n",
    "ax.scatter(\n",
    "    np.ones(9), y * 100, color=\"k\", marker=\"o\", s=5, label=\"Top 10 runs\\nval. set\"\n",
    ")\n",
    "for i in range(3, 8, 2):\n",
    "    y = np.array(fev_dict[i][1:10])\n",
    "    ax.scatter(i * np.ones(9), y * 100, color=\"k\", marker=\"o\", s=5, label=None)\n",
    "\n",
    "# best runs on val set: larger dot\n",
    "ax.scatter(\n",
    "    np.arange(1, 8, 2),\n",
    "    max_valset_fev * 100,\n",
    "    color=\"k\",\n",
    "    marker=\"o\",\n",
    "    s=25,\n",
    "    label=\"Best run\\nval. set\",\n",
    ")\n",
    "\n",
    "# used for analysis; blue dot\n",
    "ax.scatter(\n",
    "    1,\n",
    "    fev_dict[1][0] * 100,\n",
    "    color=\"xkcd:Blue\",\n",
    "    marker=\"o\",\n",
    "    s=50,\n",
    "    label=\"Used for\\nanalysis\",\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy (% FEV on test set)\")\n",
    "ax.set_yticks(np.arange(47, 50.1))\n",
    "\n",
    "ax.set_xticks(np.arange(1, 8, 2))\n",
    "norm_pool_size_deg = (np.arange(1, 8, 2) * 5 + 12) / 35  # px / 35ppd\n",
    "xlabels = np.round(norm_pool_size_deg, 2)\n",
    "ax.set_xticklabels(xlabels.astype(str))\n",
    "ax.set_xlabel(\"Size of normalization pool (deg)\")\n",
    "\n",
    "ax.legend(frameon=False, loc=\"lower left\")\n",
    "\n",
    "sns.despine(trim=True, offset=5)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Spatial normalization pool\n",
    " *For this to work, you first have to run the cell \"Get and save FEV performance on test set\"\n",
    " in the divisive_3x3_surround_net, divisive_5x5_surround_net, and divisive_7x7_surround_net analysis jupyter notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for surround_size, path in args.surround_path_dict.items():\n",
    "    run_no = pd.read_csv(os.path.join(path, \"df_best.csv\")).run_no.values[0]\n",
    "    u = helpers.pkl_load(run_no, \"u.pkl\", os.path.join(path, args.weights_path))\n",
    "    u = np.abs(u)\n",
    "\n",
    "    no_rows = 2\n",
    "    no_columns = 32\n",
    "    fig, axes = plt.subplots(no_rows, no_columns, figsize=(no_columns * 1, no_rows * 1))\n",
    "    for c in range(no_columns):\n",
    "        for r in range(no_rows):\n",
    "            ax = axes[r, c]\n",
    "            _ = ax.imshow(u[:, :, 0, c, r], cmap=\"Greys\", vmin=0)\n",
    "            ax.tick_params(\n",
    "                which=\"both\",\n",
    "                bottom=False,\n",
    "                labelbottom=False,\n",
    "                left=False,\n",
    "                labelleft=False,\n",
    "            )\n",
    "    plt.show()\n",
    "\n",
    "    uavg = np.average(u, axis=(-2, -1))\n",
    "    plt.imshow(uavg[:, :, 0], cmap=\"Greys\", vmin=0)\n",
    "    plt.tick_params(\n",
    "                which=\"both\",\n",
    "                bottom=False,\n",
    "                labelbottom=False,\n",
    "                left=False,\n",
    "                labelleft=False,\n",
    "            )\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
