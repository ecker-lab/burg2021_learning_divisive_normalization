{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "import divisivenormalization.analysis as analysis\n",
    "import divisivenormalization.utils as helpers\n",
    "from divisivenormalization.data import Dataset, MonkeySubDataset\n",
    "\n",
    "helpers.config_ipython()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "# adjust sns paper context rc parameters\n",
    "font_size = 8\n",
    "rc_dict = {\n",
    "    \"font.size\": font_size,\n",
    "    \"axes.titlesize\": font_size,\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"figure.figsize\": (helpers.cm2inch(8), helpers.cm2inch(8)),\n",
    "    \"figure.dpi\": 300,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"savefig.transparent\": True,\n",
    "    \"savefig.bbox_inches\": \"tight\",\n",
    "}\n",
    "sns.set_context(\"paper\", rc=rc_dict)\n",
    "\n",
    "\n",
    "class args:\n",
    "    num_best = 10\n",
    "    num_val = 10\n",
    "    fname_best_csv = \"df_best.csv\"\n",
    "    fname_val_csv = \"df_val.csv\"\n",
    "    weights_path = \"weights\"\n",
    "    train_logs_path = \"train_logs\"\n",
    "    orientation_binsize = np.deg2rad(10)\n",
    "    stim_full_size = 140  # full size of stimulus w/o subsampling and cropping\n",
    "    stim_subsample = 2\n",
    "    oriented_threshold = 0.125\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"results.csv\")\n",
    "# Save a simplified version of the csv file, sorted by validation set performance\n",
    "df_plain = helpers.simplify_df(results_df)\n",
    "df_plain.to_csv(\"results_plain.csv\")\n",
    "\n",
    "data_dict = Dataset.get_clean_data()\n",
    "data = MonkeySubDataset(data_dict, seed=1000, train_frac=0.8, subsample=2, crop=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Get and save FEV performance on test set\n",
    " Use the 10 best models for analysis.\n",
    " Split the csv files accordingly. Also, extract some weights to be used for later analysis and save\n",
    " them as pickle. As this operation requires model loading, we do it only if it was not done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_best = pd.read_csv(args.fname_best_csv)\n",
    "    logging.info(\"loaded data from \" + args.fname_best_csv)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    df_best = df_plain[0 : args.num_best].copy()\n",
    "\n",
    "    fev_lst = []\n",
    "    for i in range(args.num_best):\n",
    "        run_no = df_best.iloc[i][\"run_no\"]\n",
    "        logging.info(\"load run no \" + str(run_no))\n",
    "        model = helpers.load_dn_nonspecific_model(run_no, results_df, data, args.train_logs_path)\n",
    "\n",
    "        fev = model.evaluate_fev_testset()\n",
    "        fev_lst.append(fev)\n",
    "\n",
    "        feve = model.evaluate_fev_testset_per_neuron()\n",
    "        helpers.pkl_dump(feve, run_no, \"feve.pkl\", args.weights_path)\n",
    "\n",
    "        # get weights and normalization input\n",
    "        (\n",
    "            features_chanfirst,\n",
    "            p,\n",
    "            pooled,\n",
    "            readout_feat,\n",
    "            u,\n",
    "            v,\n",
    "            dn_exponent,\n",
    "        ) = helpers.get_weights(model)\n",
    "\n",
    "        norm_input = analysis.norm_input(pooled, p)\n",
    "\n",
    "        helpers.pkl_dump(features_chanfirst, run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(p, run_no, \"p.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(pooled, run_no, \"pooled.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(norm_input, run_no, \"norm_input.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(readout_feat, run_no, \"readout_feat_w.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(u, run_no, \"u.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(v, run_no, \"v.pkl\", args.weights_path)\n",
    "        helpers.pkl_dump(dn_exponent, run_no, \"dn_exponent.pkl\", args.weights_path)\n",
    "\n",
    "    df_best[\"fev\"] = fev_lst\n",
    "    df_best.to_csv(args.fname_best_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fev = df_best.fev.values * 100\n",
    "print(\"Mean FEV\", fev.mean())\n",
    "print(\"SEM\", stats.sem(fev, ddof=1))\n",
    "print(\"max FEV\", fev.max())\n",
    "print(\"FEV of model with max correlation on validation set\", fev[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Similarly oriented features contribute stronger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_input_lst, dissim_input_lst = [], []\n",
    "for i in range(args.num_best):\n",
    "    run_no = df_best.iloc[i].run_no\n",
    "    features = helpers.pkl_load(run_no, \"features_chanfirst.pkl\", args.weights_path)\n",
    "    norm_input = helpers.pkl_load(run_no, \"norm_input.pkl\", args.weights_path)\n",
    "\n",
    "    angles = analysis.angles_circ_var(features, args.oriented_threshold)\n",
    "    angles_diff = analysis.angle_diff(angles)\n",
    "    unor_mask, sim_mask, dissim_mask = analysis.orientation_masks(angles_diff)\n",
    "    sim_input = np.sum(norm_input[sim_mask])\n",
    "    dissim_input = np.sum(norm_input[dissim_mask])\n",
    "\n",
    "    sim_input_lst.append(sim_input)\n",
    "    dissim_input_lst.append(dissim_input)\n",
    "\n",
    "fractions = [s / d for s, d in zip(sim_input_lst, dissim_input_lst)]\n",
    "fraction_err = stats.sem(fractions, ddof=0)\n",
    "mean = np.average(fractions)\n",
    "conf_int = analysis.compute_confidence_interval(fractions)\n",
    "\n",
    "print(\"Similar norm. input divided by dissimilar input\", np.round(mean, 2))\n",
    "print(\"Confidence interval\", np.round(conf_int, 2))\n",
    "print(\"Plus/minus\", np.round(mean - conf_int[0], 2))\n",
    "print(stats.wilcoxon(sim_input_lst, dissim_input_lst))\n",
    "print(\"Cohen's d\", np.round(analysis.cohens_d(sim_input_lst, dissim_input_lst), 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
